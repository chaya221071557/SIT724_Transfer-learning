{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvqUhLX6Wf96",
        "outputId": "ecd37d57-f46d-40a9-b69e-36bac8687124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.4.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import time"
      ],
      "metadata": {
        "id": "fens7SSAWuan"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2a1zr63Wyqj",
        "outputId": "1597688b-9c1c-418e-9517-7c3ee07c1944"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional block with Batch Normalization and Leaky ReLU\n",
        "def conv_batch(in_channels, out_channels, kernel_size, stride, padding=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.LeakyReLU(0.1)\n",
        "    )"
      ],
      "metadata": {
        "id": "IfD5_fPpW1i_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dark Residual Block\n",
        "class DarkResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(DarkResidualBlock, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            conv_batch(in_channels, in_channels // 2, kernel_size=1, stride=1, padding=0),\n",
        "            conv_batch(in_channels // 2, in_channels, kernel_size=3, stride=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.layer(x)"
      ],
      "metadata": {
        "id": "0L7ZmdUQW5yU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a layer with multiple residual blocks\n",
        "def make_layer(block, in_channels, num_blocks):\n",
        "    layers = []\n",
        "    for _ in range(num_blocks):\n",
        "        layers.append(block(in_channels))\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "lMRTAzoTW7Wh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Darknet-53 Model\n",
        "class Darknet53(nn.Module):\n",
        "    def __init__(self, num_classes=31):\n",
        "        super(Darknet53, self).__init__()\n",
        "        self.conv1 = conv_batch(3, 32, kernel_size=3, stride=1)\n",
        "        self.conv2 = conv_batch(32, 64, kernel_size=3, stride=2)\n",
        "        self.residual_block1 = make_layer(DarkResidualBlock, in_channels=64, num_blocks=1)\n",
        "\n",
        "        self.conv3 = conv_batch(64, 128, kernel_size=3, stride=2)\n",
        "        self.residual_block2 = make_layer(DarkResidualBlock, in_channels=128, num_blocks=2)\n",
        "\n",
        "        self.conv4 = conv_batch(128, 256, kernel_size=3, stride=2)\n",
        "        self.residual_block3 = make_layer(DarkResidualBlock, in_channels=256, num_blocks=8)\n",
        "\n",
        "        self.conv5 = conv_batch(256, 512, kernel_size=3, stride=2)\n",
        "        self.residual_block4 = make_layer(DarkResidualBlock, in_channels=512, num_blocks=8)\n",
        "\n",
        "        self.conv6 = conv_batch(512, 1024, kernel_size=3, stride=2)\n",
        "        self.residual_block5 = make_layer(DarkResidualBlock, in_channels=1024, num_blocks=4)\n",
        "\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.residual_block1(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.residual_block2(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.residual_block3(x)\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        x = self.residual_block4(x)\n",
        "\n",
        "        x = self.conv6(x)\n",
        "        x = self.residual_block5(x)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "CzoG8Ot7W-OY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Function to load pre-trained weights using NumPy\n",
        "def load_darknet_weights(model, weights_path):\n",
        "    with open(weights_path, 'rb') as f:\n",
        "        header = f.read(16)  # Skip the first 16 bytes (metadata)\n",
        "        weights = np.fromfile(f, dtype=np.float32)  # Use NumPy to read binary weights\n",
        "\n",
        "    ptr = 0\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            conv_layer = module\n",
        "\n",
        "            # Check if the layer has biases and load them if they exist\n",
        "            if conv_layer.bias is not None:\n",
        "                num_biases = conv_layer.bias.numel()\n",
        "                biases = torch.from_numpy(weights[ptr:ptr + num_biases]).view_as(conv_layer.bias.data)\n",
        "                conv_layer.bias.data.copy_(biases)\n",
        "                ptr += num_biases\n",
        "\n",
        "            # Load the weights\n",
        "            num_weights = conv_layer.weight.numel()\n",
        "            conv_weights = torch.from_numpy(weights[ptr:ptr + num_weights]).view_as(conv_layer.weight.data)\n",
        "            conv_layer.weight.data.copy_(conv_weights)\n",
        "            ptr += num_weights\n"
      ],
      "metadata": {
        "id": "i0vYmAf2XHoK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JOp4OVkXKiy",
        "outputId": "c77d9b7e-53d8-4072-fbaa-0299fdd0ef98"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to your pre-trained weights\n",
        "weights_path = '/content/drive/MyDrive/darknet53.conv.74'"
      ],
      "metadata": {
        "id": "8Cv1AIc_XTCu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model and load weights\n",
        "model = Darknet53(num_classes=31)\n",
        "load_darknet_weights(model, weights_path)"
      ],
      "metadata": {
        "id": "e8WUbDvYXXfA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Freeze early layers\n",
        "for param in model.conv1.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "Dj4-rONxXuh7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "wwr_JaqjXx2v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Blended/train', transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "PdA3dabVXzqN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Blended/val', transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "mtJ8LYBrX_4J"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)"
      ],
      "metadata": {
        "id": "9ooC6bu-YGnK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with time and additional metrics\n",
        "num_epochs = 25\n",
        "best_val_acc = 0.0\n",
        "best_val_loss = float('inf')\n",
        "best_precision = 0.0\n",
        "best_recall = 0.0\n",
        "best_f1 = 0.0"
      ],
      "metadata": {
        "id": "0kCS-PNCYJKa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "U1CZOhB2YMcQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct / total\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = 100 * correct / total\n",
        "    precision = precision_score(all_labels, all_predictions, average='macro', zero_division=1)\n",
        "    recall = recall_score(all_labels, all_predictions, average='macro',zero_division=1)\n",
        "    f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=1)\n",
        "\n",
        "    # Save the best model based on validation accuracy\n",
        "    if val_accuracy > best_val_acc:\n",
        "        best_val_acc = val_accuracy\n",
        "        best_val_loss = val_loss\n",
        "        best_precision = precision\n",
        "        best_recall = recall\n",
        "        best_f1 = f1\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/darknet53_Blended_model.pth')\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, Precision: {precision:.4f}, '\n",
        "          f'Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "minutes, seconds = divmod(elapsed_time, 60)\n",
        "\n",
        "print(f\"Training complete in {int(minutes)}m {int(seconds)}s\")\n",
        "print(f\"Best val Acc: {best_val_acc:.4f}\")\n",
        "print(f\"Best val Loss: {best_val_loss:.4f}\")\n",
        "print(f\"Best val Precision: {best_precision:.4f}\")\n",
        "print(f\"Best val Recall: {best_recall:.4f}\")\n",
        "print(f\"Best val F1 Score: {best_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyC8gUOaYQ-s",
        "outputId": "6ee57c52-21dc-4e9f-dddd-e2918ace6932"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/25], Train Loss: 1.0139, Train Acc: 72.28%, Val Loss: 0.9248, Val Acc: 78.16%, Precision: 0.8243, Recall: 0.7807, F1 Score: 0.7463\n",
            "Epoch [2/25], Train Loss: 0.7025, Train Acc: 80.53%, Val Loss: 0.6449, Val Acc: 82.47%, Precision: 0.8511, Recall: 0.8443, F1 Score: 0.8321\n",
            "Epoch [3/25], Train Loss: 0.5808, Train Acc: 83.98%, Val Loss: 0.4025, Val Acc: 90.00%, Precision: 0.9044, Recall: 0.9070, F1 Score: 0.8909\n",
            "Epoch [4/25], Train Loss: 0.4465, Train Acc: 87.51%, Val Loss: 0.4517, Val Acc: 89.25%, Precision: 0.9124, Recall: 0.9063, F1 Score: 0.9047\n",
            "Epoch [5/25], Train Loss: 0.4102, Train Acc: 88.80%, Val Loss: 0.2959, Val Acc: 92.55%, Precision: 0.9353, Recall: 0.9342, F1 Score: 0.9330\n",
            "Epoch [6/25], Train Loss: 0.3202, Train Acc: 91.26%, Val Loss: 0.3277, Val Acc: 92.52%, Precision: 0.9371, Recall: 0.9352, F1 Score: 0.9332\n",
            "Epoch [7/25], Train Loss: 0.2955, Train Acc: 91.43%, Val Loss: 0.3167, Val Acc: 90.85%, Precision: 0.9210, Recall: 0.9214, F1 Score: 0.9164\n",
            "Epoch [8/25], Train Loss: 0.2893, Train Acc: 91.71%, Val Loss: 0.3081, Val Acc: 92.55%, Precision: 0.9418, Recall: 0.9343, F1 Score: 0.9348\n",
            "Epoch [9/25], Train Loss: 0.2382, Train Acc: 93.24%, Val Loss: 0.3011, Val Acc: 92.47%, Precision: 0.9316, Recall: 0.9322, F1 Score: 0.9268\n",
            "Epoch [10/25], Train Loss: 0.2351, Train Acc: 93.24%, Val Loss: 0.2707, Val Acc: 93.25%, Precision: 0.9245, Recall: 0.9413, F1 Score: 0.9270\n",
            "Epoch [11/25], Train Loss: 0.1908, Train Acc: 94.50%, Val Loss: 0.2608, Val Acc: 94.15%, Precision: 0.9503, Recall: 0.9490, F1 Score: 0.9478\n",
            "Epoch [12/25], Train Loss: 0.1730, Train Acc: 94.67%, Val Loss: 0.2868, Val Acc: 93.43%, Precision: 0.9432, Recall: 0.9412, F1 Score: 0.9397\n",
            "Epoch [13/25], Train Loss: 0.1484, Train Acc: 95.66%, Val Loss: 0.2758, Val Acc: 93.94%, Precision: 0.9475, Recall: 0.9461, F1 Score: 0.9451\n",
            "Epoch [14/25], Train Loss: 0.1432, Train Acc: 95.78%, Val Loss: 0.2302, Val Acc: 94.69%, Precision: 0.9494, Recall: 0.9537, F1 Score: 0.9503\n",
            "Epoch [15/25], Train Loss: 0.1486, Train Acc: 95.54%, Val Loss: 0.2583, Val Acc: 94.74%, Precision: 0.9511, Recall: 0.9535, F1 Score: 0.9511\n",
            "Epoch [16/25], Train Loss: 0.1233, Train Acc: 96.24%, Val Loss: 0.2795, Val Acc: 93.79%, Precision: 0.9363, Recall: 0.9458, F1 Score: 0.9381\n",
            "Epoch [17/25], Train Loss: 0.1088, Train Acc: 96.62%, Val Loss: 0.2678, Val Acc: 94.23%, Precision: 0.9381, Recall: 0.9372, F1 Score: 0.9269\n",
            "Epoch [18/25], Train Loss: 0.0981, Train Acc: 96.83%, Val Loss: 0.3005, Val Acc: 94.12%, Precision: 0.9490, Recall: 0.9485, F1 Score: 0.9466\n",
            "Epoch [19/25], Train Loss: 0.0964, Train Acc: 97.04%, Val Loss: 0.2247, Val Acc: 95.23%, Precision: 0.9539, Recall: 0.9583, F1 Score: 0.9553\n",
            "Epoch [20/25], Train Loss: 0.0831, Train Acc: 97.52%, Val Loss: 0.2517, Val Acc: 95.10%, Precision: 0.9529, Recall: 0.9572, F1 Score: 0.9540\n",
            "Epoch [21/25], Train Loss: 0.0828, Train Acc: 97.34%, Val Loss: 0.2741, Val Acc: 94.72%, Precision: 0.9506, Recall: 0.9540, F1 Score: 0.9514\n",
            "Epoch [22/25], Train Loss: 0.0744, Train Acc: 97.68%, Val Loss: 0.2113, Val Acc: 95.59%, Precision: 0.9539, Recall: 0.9616, F1 Score: 0.9572\n",
            "Epoch [23/25], Train Loss: 0.0584, Train Acc: 98.14%, Val Loss: 0.2313, Val Acc: 95.31%, Precision: 0.9512, Recall: 0.9591, F1 Score: 0.9536\n",
            "Epoch [24/25], Train Loss: 0.0717, Train Acc: 97.84%, Val Loss: 0.2171, Val Acc: 95.80%, Precision: 0.9544, Recall: 0.9633, F1 Score: 0.9582\n",
            "Epoch [25/25], Train Loss: 0.0544, Train Acc: 98.25%, Val Loss: 0.2439, Val Acc: 95.46%, Precision: 0.9578, Recall: 0.9521, F1 Score: 0.9533\n",
            "Training complete in 110m 43s\n",
            "Best val Acc: 95.7979\n",
            "Best val Loss: 0.2171\n",
            "Best val Precision: 0.9544\n",
            "Best val Recall: 0.9633\n",
            "Best val F1 Score: 0.9582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir('/content/drive/MyDrive/'))  # Replace with your directory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_xn3PsDEHcF",
        "outputId": "60478663-46eb-4fb3-922c-c583c580cfb7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Colab Notebooks', 'Blended', 'darknet53.conv.74', 'data_224_g', 'malevis_train_val_224x224', 'darknet53_full_model.pth', 'darknet53_Blended_model.pth']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "# Prediction function\n",
        "def predict_image(image_path, model, class_names):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Image transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize image to 224x224\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Load image and apply transformations\n",
        "    image = Image.open(image_path).convert('RGB')  # Convert to RGB to ensure 3 channels\n",
        "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Move image to device\n",
        "    image = image.to(device)\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    # Get the class name\n",
        "    predicted_class = class_names[preds.item()]\n",
        "    return predicted_class\n",
        "\n",
        "# Load the saved model\n",
        "model = Darknet53(num_classes=31)  # Ensure num_classes matches your model\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/darknet53_Blended_model.pth', weights_only=False))  # Load the best saved model\n",
        "model = model.to(device)\n",
        "\n",
        "# Example usage\n",
        "class_names = train_dataset.classes  # Get class names from the training dataset\n",
        "image_path = '/content/drive/MyDrive/Blended/val/Expiro/b056acc9c32458f923a78007c44753a27139bc65resized_image.png'  # Replace with the path to your image\n",
        "predicted_class = predict_image(image_path, model, class_names)\n",
        "print(f\"Predicted class: {predicted_class}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opXNBrUCEREI",
        "outputId": "5ba488e2-31a4-454e-f5cb-e4fb798411c8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: Expiro\n"
          ]
        }
      ]
    }
  ]
}