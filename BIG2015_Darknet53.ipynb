{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIqLutYsr24E",
        "outputId": "657840f7-9a29-4be5-af68-3bedd9297de0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.4.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import time"
      ],
      "metadata": {
        "id": "qgbEjjBGsHpv"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A6cKguDsNE7",
        "outputId": "f3c75cd9-e669-48d7-85f1-1cfb9532ba29"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional block with Batch Normalization and Leaky ReLU\n",
        "def conv_batch(in_channels, out_channels, kernel_size, stride, padding=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.LeakyReLU(0.1)\n",
        "    )"
      ],
      "metadata": {
        "id": "BlSp6QknsQgO"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dark Residual Block\n",
        "class DarkResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(DarkResidualBlock, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            conv_batch(in_channels, in_channels // 2, kernel_size=1, stride=1, padding=0),\n",
        "            conv_batch(in_channels // 2, in_channels, kernel_size=3, stride=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.layer(x)"
      ],
      "metadata": {
        "id": "uJLofIlMsR8a"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a layer with multiple residual blocks\n",
        "def make_layer(block, in_channels, num_blocks):\n",
        "    layers = []\n",
        "    for _ in range(num_blocks):\n",
        "        layers.append(block(in_channels))\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "OjBU213fsYj8"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Darknet-53 Model\n",
        "class Darknet53(nn.Module):\n",
        "    def __init__(self, num_classes=9):\n",
        "        super(Darknet53, self).__init__()\n",
        "        self.conv1 = conv_batch(3, 32, kernel_size=3, stride=1)\n",
        "        self.conv2 = conv_batch(32, 64, kernel_size=3, stride=2)\n",
        "        self.residual_block1 = make_layer(DarkResidualBlock, in_channels=64, num_blocks=1)\n",
        "\n",
        "        self.conv3 = conv_batch(64, 128, kernel_size=3, stride=2)\n",
        "        self.residual_block2 = make_layer(DarkResidualBlock, in_channels=128, num_blocks=2)\n",
        "\n",
        "        self.conv4 = conv_batch(128, 256, kernel_size=3, stride=2)\n",
        "        self.residual_block3 = make_layer(DarkResidualBlock, in_channels=256, num_blocks=8)\n",
        "\n",
        "        self.conv5 = conv_batch(256, 512, kernel_size=3, stride=2)\n",
        "        self.residual_block4 = make_layer(DarkResidualBlock, in_channels=512, num_blocks=8)\n",
        "\n",
        "        self.conv6 = conv_batch(512, 1024, kernel_size=3, stride=2)\n",
        "        self.residual_block5 = make_layer(DarkResidualBlock, in_channels=1024, num_blocks=4)\n",
        "\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.residual_block1(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.residual_block2(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.residual_block3(x)\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        x = self.residual_block4(x)\n",
        "\n",
        "        x = self.conv6(x)\n",
        "        x = self.residual_block5(x)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "O3zU7sn2sag7"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Function to load pre-trained weights using NumPy\n",
        "def load_darknet_weights(model, weights_path):\n",
        "    with open(weights_path, 'rb') as f:\n",
        "        header = f.read(16)  # Skip the first 16 bytes (metadata)\n",
        "        weights = np.fromfile(f, dtype=np.float32)  # Use NumPy to read binary weights\n",
        "\n",
        "    ptr = 0\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            conv_layer = module\n",
        "\n",
        "            # Check if the layer has biases and load them if they exist\n",
        "            if conv_layer.bias is not None:\n",
        "                num_biases = conv_layer.bias.numel()\n",
        "                biases = torch.from_numpy(weights[ptr:ptr + num_biases]).view_as(conv_layer.bias.data)\n",
        "                conv_layer.bias.data.copy_(biases)\n",
        "                ptr += num_biases\n",
        "\n",
        "            # Load the weights\n",
        "            num_weights = conv_layer.weight.numel()\n",
        "            conv_weights = torch.from_numpy(weights[ptr:ptr + num_weights]).view_as(conv_layer.weight.data)\n",
        "            conv_layer.weight.data.copy_(conv_weights)\n",
        "            ptr += num_weights\n",
        "\n"
      ],
      "metadata": {
        "id": "aEP4s5-hsrpH"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgfkhhJCsyq5",
        "outputId": "65e405b2-93d4-4585-ab4e-60b0a0c98c23"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to your pre-trained weights\n",
        "weights_path = '/content/drive/MyDrive/darknet53.conv.74'"
      ],
      "metadata": {
        "id": "IcP_2W7gs6m6"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model and load weights\n",
        "model = Darknet53(num_classes=9)\n",
        "load_darknet_weights(model, weights_path)"
      ],
      "metadata": {
        "id": "86TKMJS1tBYZ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Freeze early layers\n",
        "for param in model.conv1.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "bAYHUtzHuGok"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "4Hf4_lOauSvL"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/data_224_g/train', transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "iH7wHovCuVTx"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/data_224_g/val', transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "Ee_YKY_aus2i"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)"
      ],
      "metadata": {
        "id": "rDCWK8TKuzlf"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with time and additional metrics\n",
        "num_epochs = 25\n",
        "best_val_acc = 0.0\n",
        "best_val_loss = float('inf')\n",
        "best_precision = 0.0\n",
        "best_recall = 0.0\n",
        "best_f1 = 0.0"
      ],
      "metadata": {
        "id": "XYNaxgpmu2GP"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "62XSet8D9xbT"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct / total\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = 100 * correct / total\n",
        "    precision = precision_score(all_labels, all_predictions, average='macro', zero_division=1)\n",
        "    recall = recall_score(all_labels, all_predictions, average='macro',zero_division=1)\n",
        "    f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=1)\n",
        "\n",
        "    # Save the best model based on validation accuracy\n",
        "    if val_accuracy > best_val_acc:\n",
        "        best_val_acc = val_accuracy\n",
        "        best_val_loss = val_loss\n",
        "        best_precision = precision\n",
        "        best_recall = recall\n",
        "        best_f1 = f1\n",
        "        #torch.save(model.state_dict(), 'darknet53_best_model.pth')\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, Precision: {precision:.4f}, '\n",
        "          f'Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "minutes, seconds = divmod(elapsed_time, 60)\n",
        "\n",
        "print(f\"Training complete in {int(minutes)}m {int(seconds)}s\")\n",
        "print(f\"Best val Acc: {best_val_acc:.4f}\")\n",
        "print(f\"Best val Loss: {best_val_loss:.4f}\")\n",
        "print(f\"Best val Precision: {best_precision:.4f}\")\n",
        "print(f\"Best val Recall: {best_recall:.4f}\")\n",
        "print(f\"Best val F1 Score: {best_f1:.4f}\")"
      ],
      "metadata": {
        "id": "Pr5e_NFzvAyu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dafc86d-cece-45df-e976-60c7d8923e2e"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/25], Train Loss: 0.0784, Train Acc: 97.70%, Val Loss: 0.0970, Val Acc: 97.70%, Precision: 0.9395, Recall: 0.9620, F1 Score: 0.9494\n",
            "Epoch [2/25], Train Loss: 0.0606, Train Acc: 98.15%, Val Loss: 0.1188, Val Acc: 97.15%, Precision: 0.9492, Recall: 0.9463, F1 Score: 0.9470\n",
            "Epoch [3/25], Train Loss: 0.0589, Train Acc: 98.14%, Val Loss: 0.1201, Val Acc: 97.20%, Precision: 0.9648, Recall: 0.9545, F1 Score: 0.9590\n",
            "Epoch [4/25], Train Loss: 0.0532, Train Acc: 98.21%, Val Loss: 0.1301, Val Acc: 97.33%, Precision: 0.9483, Recall: 0.9564, F1 Score: 0.9518\n",
            "Epoch [5/25], Train Loss: 0.0622, Train Acc: 98.39%, Val Loss: 0.1267, Val Acc: 97.56%, Precision: 0.9710, Recall: 0.9503, F1 Score: 0.9590\n",
            "Epoch [6/25], Train Loss: 0.0684, Train Acc: 97.75%, Val Loss: 0.1436, Val Acc: 96.65%, Precision: 0.9572, Recall: 0.9338, F1 Score: 0.9436\n",
            "Epoch [7/25], Train Loss: 0.0500, Train Acc: 98.46%, Val Loss: 0.1315, Val Acc: 97.33%, Precision: 0.9555, Recall: 0.9595, F1 Score: 0.9572\n",
            "Epoch [8/25], Train Loss: 0.0363, Train Acc: 98.98%, Val Loss: 0.1320, Val Acc: 97.20%, Precision: 0.9487, Recall: 0.9468, F1 Score: 0.9471\n",
            "Epoch [9/25], Train Loss: 0.0573, Train Acc: 98.23%, Val Loss: 0.1228, Val Acc: 97.43%, Precision: 0.9245, Recall: 0.9594, F1 Score: 0.9380\n",
            "Epoch [10/25], Train Loss: 0.0418, Train Acc: 98.58%, Val Loss: 0.1269, Val Acc: 97.29%, Precision: 0.9436, Recall: 0.9539, F1 Score: 0.9480\n",
            "Epoch [11/25], Train Loss: 0.0313, Train Acc: 98.94%, Val Loss: 0.1506, Val Acc: 97.20%, Precision: 0.9268, Recall: 0.9557, F1 Score: 0.9384\n",
            "Epoch [12/25], Train Loss: 0.0492, Train Acc: 98.53%, Val Loss: 0.1214, Val Acc: 97.15%, Precision: 0.9379, Recall: 0.9585, F1 Score: 0.9476\n",
            "Epoch [13/25], Train Loss: 0.0331, Train Acc: 98.90%, Val Loss: 0.1258, Val Acc: 97.43%, Precision: 0.9686, Recall: 0.9558, F1 Score: 0.9616\n",
            "Epoch [14/25], Train Loss: 0.0357, Train Acc: 98.87%, Val Loss: 0.1248, Val Acc: 97.43%, Precision: 0.9594, Recall: 0.9564, F1 Score: 0.9576\n",
            "Epoch [15/25], Train Loss: 0.0331, Train Acc: 98.91%, Val Loss: 0.1184, Val Acc: 97.61%, Precision: 0.9630, Recall: 0.9608, F1 Score: 0.9618\n",
            "Epoch [16/25], Train Loss: 0.0330, Train Acc: 98.96%, Val Loss: 0.1385, Val Acc: 97.33%, Precision: 0.9279, Recall: 0.9598, F1 Score: 0.9418\n",
            "Epoch [17/25], Train Loss: 0.0305, Train Acc: 99.01%, Val Loss: 0.1256, Val Acc: 97.70%, Precision: 0.9507, Recall: 0.9646, F1 Score: 0.9573\n",
            "Epoch [18/25], Train Loss: 0.0209, Train Acc: 99.34%, Val Loss: 0.1451, Val Acc: 97.29%, Precision: 0.9687, Recall: 0.9514, F1 Score: 0.9587\n",
            "Epoch [19/25], Train Loss: 0.0344, Train Acc: 98.90%, Val Loss: 0.1629, Val Acc: 96.78%, Precision: 0.9175, Recall: 0.9519, F1 Score: 0.9315\n",
            "Epoch [20/25], Train Loss: 0.0346, Train Acc: 98.91%, Val Loss: 0.1372, Val Acc: 97.33%, Precision: 0.9672, Recall: 0.9562, F1 Score: 0.9610\n",
            "Epoch [21/25], Train Loss: 0.0266, Train Acc: 99.10%, Val Loss: 0.1536, Val Acc: 97.38%, Precision: 0.9559, Recall: 0.9583, F1 Score: 0.9569\n",
            "Epoch [22/25], Train Loss: 0.0190, Train Acc: 99.47%, Val Loss: 0.1371, Val Acc: 97.33%, Precision: 0.9692, Recall: 0.9549, F1 Score: 0.9615\n",
            "Epoch [23/25], Train Loss: 0.0280, Train Acc: 99.10%, Val Loss: 0.1588, Val Acc: 97.61%, Precision: 0.9629, Recall: 0.9602, F1 Score: 0.9613\n",
            "Epoch [24/25], Train Loss: 0.0201, Train Acc: 99.33%, Val Loss: 0.1155, Val Acc: 97.61%, Precision: 0.9307, Recall: 0.9623, F1 Score: 0.9446\n",
            "Epoch [25/25], Train Loss: 0.0305, Train Acc: 99.08%, Val Loss: 0.1422, Val Acc: 97.43%, Precision: 0.9385, Recall: 0.9615, F1 Score: 0.9489\n",
            "Training complete in 23m 43s\n",
            "Best val Acc: 97.7022\n",
            "Best val Loss: 0.0970\n",
            "Best val Precision: 0.9395\n",
            "Best val Recall: 0.9620\n",
            "Best val F1 Score: 0.9494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, '/content/drive/MyDrive/darknet53_full_model.pth')  # Saves the entire model\n",
        "\n"
      ],
      "metadata": {
        "id": "GIRoMIweLfob"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir('/content/drive/MyDrive/'))  # Replace with your directory\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvt9xWXMLum2",
        "outputId": "0ddd5db2-06df-40fa-f1db-de7c4bde2780"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Colab Notebooks', 'Blended', 'darknet53.conv.74', 'data_224_g', 'malevis_train_val_224x224', 'darknet53_full_model.pth']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "# Prediction function\n",
        "def predict_image(image_path, model, class_names):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Image transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize image to 224x224\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Load image and apply transformations\n",
        "    image = Image.open(image_path).convert('RGB')  # Convert to RGB to ensure 3 channels\n",
        "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Move image to device\n",
        "    image = image.to(device)\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    # Get the class name\n",
        "    predicted_class = class_names[preds.item()]\n",
        "    return predicted_class\n",
        "\n",
        "# Load the saved model\n",
        "model = Darknet53(num_classes=9)  # Ensure num_classes matches your model\n",
        "model = torch.load('/content/drive/MyDrive/darknet53_full_model.pth', weights_only=False)\n",
        "#model.load_state_dict(torch.load('/content/drive/MyDrive/darknet53_full_model.pth'))  # Load the best saved model\n",
        "model = model.to(device)\n",
        "\n",
        "# Example usage\n",
        "class_names = train_dataset.classes  # Get class names from the training dataset\n",
        "image_path = '/content/drive/MyDrive/data_224_g/val/Lollipop/HMY_0gHs6DEouiCPAcmWFrTX_2.png'  # Replace with the path to your image\n",
        "predicted_class = predict_image(image_path, model, class_names)\n",
        "print(f\"Predicted class: {predicted_class}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z9iysEJKXFz",
        "outputId": "2da617eb-30c8-482f-bb7c-27f252621a19"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: Lollipop\n"
          ]
        }
      ]
    }
  ]
}